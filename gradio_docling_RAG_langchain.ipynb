{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Gradio interface for Docking + Langchain + Milvus + Mistral + all-MiniLM-L6-v2\n",
        "\n",
        "**RAG**\n",
        "\n",
        "**Supports multiple file uploads** (PDF, Images, HTML, PPTx)\n",
        "\n",
        "\n",
        "Developed by: Partha Prati Ray, https://github.com/ParthaPRay"
      ],
      "metadata": {
        "id": "amSj0iWN9UWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sk4Pe1ZjxBU",
        "outputId": "12919860-36f9-4eb5-af6a-b8ba7bf44c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m343.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.1/113.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m543.2/543.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.2/912.2 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.8/286.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# requirements for this example:\n",
        "%pip install -qq docling docling-core python-dotenv langchain-text-splitters langchain-huggingface langchain-milvus gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For local machine only\n",
        "# import os\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "\n",
        "# load_dotenv()\n"
      ],
      "metadata": {
        "id": "yMoQmS_Vj_Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For colab only\n",
        "\n",
        "# from google.colab import userdata\n",
        "# HF_API_KEY = userdata.get('HF_TOKEN')\n",
        "\n",
        "# HF_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JjkZz08hkAtJ",
        "outputId": "82bd8400-a093-4b8e-de40-eaf4084b4391"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Supports only PDF Documents\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "import time\n",
        "from tempfile import NamedTemporaryFile, TemporaryDirectory\n",
        "\n",
        "# ---------------------------\n",
        "# Imports from your RAG code\n",
        "# ---------------------------\n",
        "\n",
        "from typing import Iterator\n",
        "from typing import Iterable\n",
        "\n",
        "# docling & docling_core\n",
        "from langchain_core.document_loaders import BaseLoader\n",
        "from langchain_core.documents import Document as LCDocument\n",
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "# text splitters\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# embeddings\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# vector store (Milvus)\n",
        "from langchain_milvus import Milvus\n",
        "\n",
        "# LLM\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# RAG\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# ---------------------------\n",
        "# Custom Docling PDF Loader\n",
        "# ---------------------------\n",
        "class DoclingPDFLoader(BaseLoader):\n",
        "    def __init__(self, file_path: str | list[str]) -> None:\n",
        "        self._file_paths = file_path if isinstance(file_path, list) else [file_path]\n",
        "        self._converter = DocumentConverter()\n",
        "\n",
        "    def lazy_load(self) -> Iterator[LCDocument]:\n",
        "        for source in self._file_paths:\n",
        "            dl_doc = self._converter.convert(source).document\n",
        "            text = dl_doc.export_to_markdown(strict_text=True)  # plain text\n",
        "            yield LCDocument(page_content=text)\n",
        "\n",
        "# ---------------------------\n",
        "# Global states\n",
        "# ---------------------------\n",
        "splitted_docs = None\n",
        "rag_chain = None\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# RAG Pipeline (to be built after splitting)\n",
        "# ---------------------------\n",
        "def build_rag_chain(docs):\n",
        "    \"\"\"\n",
        "    Build a RAG pipeline from splitted docs:\n",
        "    1. Create embeddings\n",
        "    2. Create vector store\n",
        "    3. Create retriever\n",
        "    4. Build chain\n",
        "    \"\"\"\n",
        "    # Example embeddings\n",
        "    HF_EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=HF_EMBED_MODEL_ID)\n",
        "\n",
        "    # Build Milvus vector store\n",
        "    tmp_dir = TemporaryDirectory()\n",
        "    MILVUS_URI = f\"{tmp_dir.name}/milvus_demo.db\"\n",
        "    vectorstore = Milvus.from_documents(\n",
        "        docs,\n",
        "        embeddings,\n",
        "        connection_args={\"uri\": MILVUS_URI},\n",
        "        drop_old=True,\n",
        "    )\n",
        "\n",
        "    # Build a retriever\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = PromptTemplate.from_template(\n",
        "        \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\n\"\n",
        "        \"Given the context information and not prior knowledge, answer the query.\\n\"\n",
        "        \"Query: {question}\\nAnswer:\\n\"\n",
        "    )\n",
        "\n",
        "    # Example LLM\n",
        "    # For colab usage, you might read token from google.colab.userdata.get('HF_TOKEN')\n",
        "\n",
        "    #HF_API_KEY = os.environ.get(\"HF_TOKEN\", \"YOUR_HF_TOKEN_HERE\")\n",
        "\n",
        "    HF_API_KEY = \"YOUR_OWN_API\"   # Use your HuggingFace Token Key Here\n",
        "\n",
        "    HF_LLM_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id=HF_LLM_MODEL_ID,\n",
        "        huggingfacehub_api_token=HF_API_KEY,\n",
        "    )\n",
        "\n",
        "    # build the chain\n",
        "    def format_docs(selected_docs: Iterable[LCDocument]):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in selected_docs)\n",
        "\n",
        "    rag_chain_temp = (\n",
        "        {\n",
        "            \"context\": retriever | format_docs,\n",
        "            \"question\": RunnablePassthrough()\n",
        "        }\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return rag_chain_temp\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Function: Text Splitting\n",
        "# ---------------------------\n",
        "def text_splitting(files):\n",
        "    \"\"\"\n",
        "    This function:\n",
        "    1. Saves uploaded PDFs to temporary paths\n",
        "    2. Loads them via DoclingPDFLoader\n",
        "    3. Splits them using RecursiveCharacterTextSplitter\n",
        "    4. Builds the RAG chain\n",
        "    5. Returns status updates as a generator for Gradio\n",
        "    \"\"\"\n",
        "    global splitted_docs, rag_chain\n",
        "\n",
        "    # Start progress\n",
        "    yield \"Splitting in progress...\"\n",
        "\n",
        "    # Save uploaded files to local temporary paths\n",
        "    temp_file_paths = []\n",
        "    for f in files:\n",
        "        with NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "            tmp.write(f)  # <-- Directly write bytes\n",
        "            temp_file_paths.append(tmp.name)\n",
        "\n",
        "\n",
        "    # Create Docling loader\n",
        "    loader = DoclingPDFLoader(file_path=temp_file_paths)\n",
        "\n",
        "    # Create text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "    # Load the docs\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Split documents\n",
        "    splitted_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Build RAG chain\n",
        "    rag_chain = build_rag_chain(splitted_docs)\n",
        "\n",
        "    # End progress\n",
        "    yield \"Split complete!\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Function: RAG Q&A\n",
        "# ---------------------------\n",
        "def ask_question(question):\n",
        "    \"\"\"\n",
        "    Use the built rag_chain to answer a question.\n",
        "    \"\"\"\n",
        "    global splitted_docs, rag_chain\n",
        "    if not splitted_docs or not rag_chain:\n",
        "        return \"No splitted docs available. Please upload and split first!\"\n",
        "\n",
        "    # run the chain\n",
        "    answer = rag_chain.invoke(question)\n",
        "    return answer\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Build Gradio UI\n",
        "# ---------------------------\n",
        "def build_app():\n",
        "    with gr.Blocks() as demo:\n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"Upload & Split\"):\n",
        "                gr.Markdown(\"## Upload your PDFs and Split\")\n",
        "                file_upload = gr.File(\n",
        "                    label=\"Upload Files\",\n",
        "                    file_count=\"multiple\",\n",
        "                    type=\"binary\"\n",
        "                )\n",
        "                status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
        "                split_button = gr.Button(\"Split Documents\")\n",
        "\n",
        "                # We use .click with a generator function to stream status updates\n",
        "                split_button.click(\n",
        "                    fn=text_splitting,\n",
        "                    inputs=[file_upload],\n",
        "                    outputs=status_box\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"RAG Q&A\"):\n",
        "                gr.Markdown(\"## Ask a question on your splitted documents\")\n",
        "                sample_question = gr.Textbox(\n",
        "                    label=\"Question\",\n",
        "                    placeholder=\"Enter your question...\",\n",
        "                    value=\"Does Docling implement a linear pipeline of operations?\"\n",
        "                )\n",
        "                answer_box = gr.Textbox(label=\"Answer\", interactive=False)\n",
        "                ask_button = gr.Button(\"Ask\")\n",
        "\n",
        "                ask_button.click(\n",
        "                    fn=ask_question,\n",
        "                    inputs=sample_question,\n",
        "                    outputs=answer_box\n",
        "                )\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = build_app()\n",
        "    app.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QlbjNmhV1jEF",
        "outputId": "4f5adb07-d109-40d6-fac4-44676b51257f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://aa3c8ef78824920dc2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://aa3c8ef78824920dc2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:docling.datamodel.document:Input document tmp_0sro_e9.pdf does not match any allowed format.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 714, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2047, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1606, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 714, in async_iteration\n",
            "    return await anext(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 708, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 691, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 852, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"<ipython-input-12-c8589db0b810>\", line 152, in text_splitting\n",
            "    docs = loader.load()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/document_loaders/base.py\", line 31, in load\n",
            "    return list(self.lazy_load())\n",
            "  File \"<ipython-input-12-c8589db0b810>\", line 48, in lazy_load\n",
            "    dl_doc = self._converter.convert(source).document\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_validate_call.py\", line 38, in wrapper_function\n",
            "    return wrapper(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_validate_call.py\", line 111, in __call__\n",
            "    res = self.__pydantic_validator__.validate_python(pydantic_core.ArgsKwargs(args, kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/docling/document_converter.py\", line 189, in convert\n",
            "    return next(all_res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/docling/document_converter.py\", line 210, in convert_all\n",
            "    for conv_res in conv_res_iter:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/docling/document_converter.py\", line 245, in _convert\n",
            "    for item in map(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/docling/document_converter.py\", line 290, in _process_document\n",
            "    raise ConversionError(error_message)\n",
            "docling.exceptions.ConversionError: File format not allowed: tmp_0sro_e9.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7864 <> https://aa3c8ef78824920dc2.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Supports PDF, Image, PPTx, HTML,\n",
        "\n",
        "# .TXT, .MD, .asciidoc to be tested\n",
        "\n",
        "## Word .docx, excel .xlsx are not supported\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "from tempfile import NamedTemporaryFile, TemporaryDirectory\n",
        "\n",
        "# Docling & LangChain imports\n",
        "from docling.document_converter import (\n",
        "    DocumentConverter,\n",
        "    PdfFormatOption,\n",
        "    WordFormatOption,\n",
        ")\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
        "#from docling.backend.msword_backend import MsWordDocumentBackend\n",
        "\n",
        "from docling.pipeline.simple_pipeline import SimplePipeline\n",
        "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
        "\n",
        "from langchain_core.documents import Document as LCDocument\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_milvus import Milvus\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1) Configure Docling's DocumentConverter for multi-format\n",
        "# ------------------------------------------------------------------\n",
        "doc_converter = (DocumentConverter(\n",
        "    allowed_formats=[\n",
        "        InputFormat.PDF,\n",
        "        InputFormat.IMAGE,    # for images (requires OCR libs if actual text extraction is desired)\n",
        "        #InputFormat.DOCX,     # Word\n",
        "        InputFormat.HTML,     # HTML\n",
        "        InputFormat.PPTX,     # PowerPoint\n",
        "      #  InputFormat.ASCIIDOC, # AsciiDoc\n",
        "      #  InputFormat.MD,       # Markdown\n",
        "    ],\n",
        "    format_options={\n",
        "        InputFormat.PDF: PdfFormatOption(\n",
        "            pipeline_cls=StandardPdfPipeline,\n",
        "            backend=PyPdfiumDocumentBackend\n",
        "        ),\n",
        "        InputFormat.DOCX: WordFormatOption(  # word not supported yet\n",
        "            pipeline_cls=SimplePipeline #, backend=MsWordDocumentBackend\n",
        "        ),\n",
        "        # You can add further format_options here if needed,\n",
        "        # e.g., PPTX -> PptxFormatOption, etc.\n",
        "    },\n",
        "  )\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2) A simple \"loader\" that uses this doc_converter\n",
        "# ------------------------------------------------------------------\n",
        "class DoclingLoader:\n",
        "    def __init__(self, file_paths: list[str]):\n",
        "        # Accept either single path or list\n",
        "        if isinstance(file_paths, str):\n",
        "            file_paths = [file_paths]\n",
        "        self.file_paths = file_paths\n",
        "\n",
        "    def load_docs(self):\n",
        "        docs = []\n",
        "        for fp in self.file_paths:\n",
        "            # Let Docling auto-detect & convert\n",
        "            conv_result = doc_converter.convert(fp)  # returns a ConversionResult\n",
        "            dl_doc = conv_result.document            # the actual Docling Document\n",
        "            text = dl_doc.export_to_markdown(strict_text=True)\n",
        "            docs.append(LCDocument(page_content=text))\n",
        "        return docs\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3) Global state for splitted docs and RAG chain\n",
        "# ------------------------------------------------------------------\n",
        "splitted_docs = None\n",
        "rag_chain = None\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4) Build RAG chain\n",
        "# ------------------------------------------------------------------\n",
        "def build_rag_chain(docs):\n",
        "    # Create embeddings\n",
        "    HF_EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=HF_EMBED_MODEL_ID)\n",
        "\n",
        "    # Temporary local Milvus DB\n",
        "    tmp_dir = TemporaryDirectory()\n",
        "    MILVUS_URI = f\"{tmp_dir.name}/milvus_demo.db\"\n",
        "\n",
        "    # Build vector store\n",
        "    vectorstore = Milvus.from_documents(\n",
        "        docs,\n",
        "        embeddings,\n",
        "        connection_args={\"uri\": MILVUS_URI},\n",
        "        drop_old=True,\n",
        "    )\n",
        "\n",
        "    # Create retriever\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    # Prompt\n",
        "    prompt = PromptTemplate.from_template(\n",
        "        \"Context information is below.\\n---------------------\\n{context}\\n\"\n",
        "        \"---------------------\\nGiven the context information and not prior knowledge, \"\n",
        "        \"answer the query.\\nQuery: {question}\\nAnswer:\\n\"\n",
        "    )\n",
        "\n",
        "    # Setup HuggingFace LLM\n",
        "    #HF_API_KEY = os.environ.get(\"HF_TOKEN\", \"YOUR_HF_TOKEN_HERE\")\n",
        "\n",
        "    HF_API_KEY = \"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\"   # Use your HuggingFace Token Key Here\n",
        "\n",
        "    HF_LLM_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id=HF_LLM_MODEL_ID,\n",
        "        huggingfacehub_api_token=HF_API_KEY,\n",
        "    )\n",
        "\n",
        "    def format_docs(selected_docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in selected_docs)\n",
        "\n",
        "    # Build pipeline (RAG chain)\n",
        "    rag_chain_temp = (\n",
        "        {\n",
        "            \"context\": retriever | format_docs,\n",
        "            \"question\": RunnablePassthrough()\n",
        "        }\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return rag_chain_temp\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5) Gradio callback: Text Splitting\n",
        "# ------------------------------------------------------------------\n",
        "def text_splitting(files):\n",
        "    \"\"\"\n",
        "    1. Save the uploaded bytes to temporary disk paths.\n",
        "    2. Convert them via DoclingLoader (doc_converter).\n",
        "    3. Split them with LangChain.\n",
        "    4. Build the RAG chain.\n",
        "    5. Yield intermediate status to Gradio.\n",
        "    \"\"\"\n",
        "    global splitted_docs, rag_chain\n",
        "\n",
        "    yield \"Splitting in progress...\"\n",
        "\n",
        "    temp_file_paths = []\n",
        "    for f in files:\n",
        "        # If type=\"binary\" in gr.File, 'f' is raw bytes\n",
        "        with NamedTemporaryFile(delete=False) as tmp:\n",
        "            tmp.write(f)\n",
        "            temp_file_paths.append(tmp.name)\n",
        "\n",
        "    # Load docs using Docling\n",
        "    loader = DoclingLoader(temp_file_paths)\n",
        "    docs = loader.load_docs()\n",
        "\n",
        "    # Split\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    splitted_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Build RAG chain\n",
        "    rag_chain = build_rag_chain(splitted_docs)\n",
        "\n",
        "    yield \"Split complete!\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 6) Gradio callback: RAG Q&A\n",
        "# ------------------------------------------------------------------\n",
        "def ask_question(question):\n",
        "    global splitted_docs, rag_chain\n",
        "    if not splitted_docs or not rag_chain:\n",
        "        return \"No splitted docs available. Please upload and split first!\"\n",
        "    return rag_chain.invoke(question)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7) Build Gradio UI\n",
        "# ------------------------------------------------------------------\n",
        "def build_app():\n",
        "    with gr.Blocks() as demo:\n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"Upload & Split\"):\n",
        "                gr.Markdown(\n",
        "                    \"### Upload your files (PDF, HTML, PPTX, Images)\"\n",
        "                )\n",
        "                file_upload = gr.File(\n",
        "                    label=\"Upload Files\",\n",
        "                    file_count=\"multiple\",\n",
        "                    type=\"binary\"\n",
        "                )\n",
        "                status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
        "                split_button = gr.Button(\"Split Documents\")\n",
        "\n",
        "                # This streams status messages (\"Splitting in progress...\", \"Split complete!\")\n",
        "                split_button.click(\n",
        "                    fn=text_splitting,\n",
        "                    inputs=[file_upload],\n",
        "                    outputs=status_box\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"RAG Q&A\"):\n",
        "                gr.Markdown(\"### Ask a question about your splitted documents\")\n",
        "                question_box = gr.Textbox(\n",
        "                    label=\"Question\",\n",
        "                    value=\"Does Docling implement a linear pipeline of operations?\"\n",
        "                )\n",
        "                answer_box = gr.Textbox(label=\"Answer\", interactive=False)\n",
        "                ask_button = gr.Button(\"Ask\")\n",
        "\n",
        "                ask_button.click(\n",
        "                    fn=ask_question,\n",
        "                    inputs=question_box,\n",
        "                    outputs=answer_box\n",
        "                )\n",
        "    return demo\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 8) Launch the Gradio app\n",
        "# ------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    app = build_app()\n",
        "    app.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "o2pKIrkV_HNm",
        "outputId": "d8ebe0ab-36f7-4edc-f976-db6fa966270f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d387d6d20da41e5235.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://d387d6d20da41e5235.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:docling.backend.html_backend:detected nested tables: skipping for now\n",
            "WARNING:docling.backend.html_backend:detected nested tables: skipping for now\n"
          ]
        }
      ]
    }
  ]
}